import os
import sys
import tempfile
import requests
import speech_recognition as sr
import contextlib
import pygame
import noisereduce as nr
import soundfile as sf
import torch, cv2
import time

from transformers import pipeline
from ultralytics import YOLO
from gtts import gTTS


# ========== ì„¤ì • ==========
USE_GPU = torch.cuda.is_available()
DEVICE = 0 if USE_GPU else -1
WAKE_WORDS = ["í—¬ë¡œ", "hi", "í•˜ì´", "ì•ˆë…•"]
EXIT_WORDS = ["ì—†ì–´", "ëì–´", "ì•„ë‹ˆ"]
MUSIC_PATH = "/home/naseungwon/reachy_mini/no-copyright-music-1.mp3"

# [ì´ˆê¸°í™”: YOLO ëª¨ë¸ ë¡œë“œ]
EMOTION_MODEL_PATH = "/home/naseungwon/reachy_mini/yolo_detect/best.pt"
emotion_model = YOLO(EMOTION_MODEL_PATH)
emotion_labels = ['anger', 'fear', 'happy', 'neutral', 'sad']

# ========== ì´ˆê¸°í™” ==========
pygame.mixer.init()
pygame.mixer.set_num_channels(8)  # ì±„ë„ ì—¬ìœ  í™•ë³´
TTS_CH = pygame.mixer.Channel(7)  # TTS ì „ìš© ì±„ë„ í•˜ë‚˜ ì¡ê¸°

print("ğŸ§  ë¦¬ì¹˜ ë¯¸ë‹ˆ ë¡œë”© ì¤‘...")
stt_pipeline = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3", device=DEVICE)

model = "google/flan-t5-base"
llm = pipeline("text2text-generation", model=model)

# ========== ìœ í‹¸ í•¨ìˆ˜ ==========
@contextlib.contextmanager
def suppress_stderr():
    with open(os.devnull, 'w') as fnull:
        stderr = sys.stderr
        sys.stderr = fnull
        try:
            yield
        finally:
            sys.stderr = stderr

def _wait_tts_quiet(extra_ms=250):
    """TTSê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ì—ì½” ë°©ì§€) + ì•½ê°„ì˜ ì—¬ìœ  ì‹œê°„"""
    try:
        while TTS_CH.get_busy():
            time.sleep(0.05)
    except Exception:
        pass
    time.sleep(extra_ms / 1000.0)

class _MusicDucker:
    def __init__(self, vol=0.25):
        self.vol = vol
        self.prev = None
    def __enter__(self):
        try:
            self.prev = pygame.mixer.music.get_volume()
            pygame.mixer.music.set_volume(self.vol)
        except Exception:
            pass
    def __exit__(self, exc_type, exc, tb):
        try:
            if self.prev is not None:
                pygame.mixer.music.set_volume(self.prev)
        except Exception:
            pass


# ========== ìŒì„± ì…ë ¥ ==========
def listen_audio(timeout=10, phrase_time_limit=5, filename="input.wav"):
    _wait_tts_quiet()  # ğŸ‘ˆ TTS ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ê°€ì¥ ì¤‘ìš”!)
    recognizer = sr.Recognizer()
    with sr.Microphone() as source:
        print("ğŸ¤ ìŒì„±ì„ ë“£ê³  ìˆì–´ìš”...")
        try:
            recognizer.adjust_for_ambient_noise(source, duration=0.3)
        except Exception:
            pass
        with suppress_stderr():
            audio = recognizer.listen(source, timeout=timeout, phrase_time_limit=phrase_time_limit)
            with open(filename, "wb") as f:
                f.write(audio.get_wav_data())
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {filename}")
    return filename

def clean_audio(input_path="input.wav", output_path="cleaned.wav"):
    try:
        data, rate = sf.read(input_path)
        reduced = nr.reduce_noise(y=data, sr=rate)
        sf.write(output_path, reduced, rate)
        print(f"ğŸ”‡ ë…¸ì´ì¦ˆ ì œê±° ì™„ë£Œ â†’ {output_path}")
        return output_path
    except Exception as e:
        print("âŒ ë…¸ì´ì¦ˆ ì œê±° ì‹¤íŒ¨:", e)
        return input_path

def transcribe_audio(filename="cleaned.wav"):
    result = stt_pipeline(filename)
    print("ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸:", result['text'])
    return result['text'].strip()

# ========== í…ìŠ¤íŠ¸ ì‘ë‹µ ìƒì„± ==========
def generate_response(text):
    prompt = f"ì§ˆë¬¸: {text.strip()}\nëŒ€ë‹µ:"
    result = llm(prompt, max_new_tokens=100)
    response = result[0]["generated_text"].strip()
    return response if response else "ì£„ì†¡í•´ìš”, ì˜ ì´í•´í•˜ì§€ ëª»í–ˆì–´ìš”."

# ========== ìŒì„± ì¶œë ¥ ==========
def speak(text, lang='ko', blocking=True):
    if not text.strip():
        print("âš ï¸ ìŒì„±ìœ¼ë¡œ ì¶œë ¥í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    print(f"ğŸ—£ ì‘ë‹µ: {text}")
    tts = gTTS(text=text, lang=lang)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.save(fp.name)
        snd = pygame.mixer.Sound(fp.name)

    # ìŒì•…ì„ ì‚´ì§ ì¤„ì´ê³ (TTS ì„ ëª…ë„â†‘) TTS ì „ìš© ì±„ë„ì—ì„œ ì¬ìƒ
    with _MusicDucker(vol=0.25):
        TTS_CH.play(snd)
        if blocking:
            while TTS_CH.get_busy():
                time.sleep(0.05)


# ========== ìŒì•… ì¬ìƒ ==========
def play_music():
    if os.path.exists(MUSIC_PATH):
        pygame.mixer.music.load(MUSIC_PATH)
        pygame.mixer.music.play()
        print("ğŸµ ìŒì•… ì¬ìƒ ì¤‘...")
    else:
        speak("ì£„ì†¡í•´ìš”, ìŒì•… íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”.")

def stop_music():
    if pygame.mixer.music.get_busy():
        pygame.mixer.music.stop()
        print("â¹ ìŒì•… êº¼ì§")

def speak_nonblocking(text, lang='ko'):
    tts = gTTS(text=text, lang=lang)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        tts.save(fp.name)
        sound = pygame.mixer.Sound(fp.name)
        ch = pygame.mixer.find_channel()
        if ch:
            ch.play(sound)

# ========== ë‚ ì”¨ ì •ë³´ ==========
def get_weather(city="Seoul"):
    API_KEY = "Your API KEY"
    url = f"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={API_KEY}&lang=kr&units=metric"
    res = requests.get(url)
    if res.status_code == 200:
        data = res.json()
        desc = data["weather"][0]["description"]
        temp = data["main"]["temp"]
        return f"{city}ì˜ í˜„ì¬ ë‚ ì”¨ëŠ” {desc}, ê¸°ì˜¨ì€ {temp:.1f}ë„ì…ë‹ˆë‹¤."
    else:
        return "ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆì–´ìš”."

# ê°ì • ì¸ì‹ í•¨ìˆ˜ ì •ì˜
def detect_emotion_yolo(timeout=5, show=False):
    try:
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        if not cap.isOpened():
            print("âŒ ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None

        print("ğŸ§  ê°ì • ì¸ì‹ ì¤‘...")
        start_time = time.time()
        detected_emotion = None
        device_arg = 0 if USE_GPU else 'cpu'

        while time.time() - start_time < timeout:
            ret, frame = cap.read()
            if not ret:
                continue

            results = emotion_model.predict(frame, imgsz=640, device=device_arg, verbose=False)[0]
            if len(results.boxes) > 0:
                try:
                    box = max(results.boxes, key=lambda b: (b.xyxy[0][2] - b.xyxy[0][0]) * (b.xyxy[0][3] - b.xyxy[0][1]))
                    class_id = int(box.cls.item())
                    detected_emotion = emotion_labels[class_id]

                    if show:
                        xyxy = box.xyxy[0].cpu().numpy().astype(int)
                        cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)
                        cv2.putText(frame, detected_emotion, (xyxy[0], xyxy[1] - 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
                        cv2.imshow("Emotion Detection", frame)
                        if cv2.waitKey(1) & 0xFF == ord('q'):
                            break
                    else:
                        # ë””ë²„ê·¸ ë¯¸í‘œì‹œ ëª¨ë“œì—ì„œëŠ” ì°½ì„ ë„ìš°ì§€ ì•ŠìŒ
                        pass

                    break
                except Exception as e:
                    print(f"â— ê°ì • ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue
            elif show:
                cv2.imshow("Emotion Detection", frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break

        cap.release()
        if show:
            cv2.destroyAllWindows()
        return detected_emotion

    except Exception as e:
        print(f"ğŸš¨ ê°ì • ì¸ì‹ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None


# ========== ë©”ì¸ ë£¨í”„ ==========
def main():
    print("ğŸ¤– 'ë¦¬ì¹˜ ë¯¸ë‹ˆ'ê°€ ì‚¬ìš©ìë¥¼ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ìš” ...")

    # [1] ì›¨ì´í¬ ì›Œë“œ ëŒ€ê¸°
    while True:
        path = listen_audio()
        cleaned = clean_audio(path)
        transcript = transcribe_audio(cleaned).lower()
        if any(transcript.startswith(wake) or transcript == wake for wake in WAKE_WORDS):
            speak("ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
            break

    # [2] ëª…ë ¹ ì²˜ë¦¬ ë£¨í”„ (ì§€ì†ì ìœ¼ë¡œ ëª…ë ¹ ìˆ˜ìš©)
    while True:
        path = listen_audio()
        cleaned = clean_audio(path)
        user_text = transcribe_audio(cleaned).lower()

        if any(bye in user_text for bye in EXIT_WORDS):
            speak("í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ëª…ë ¹ ì²˜ë¦¬
        if "ë‚ ì”¨" in user_text:
            response = get_weather()
            speak(response)

        # ê°ì • ë¶„ì„ ë¶„ê¸° ì¶”ê°€
        elif "ê¸°ë¶„" in user_text or "ë‚´ ê¸°ë¶„" in user_text:
            speak("ê¸°ë¶„ì´ ì–´ë–¤ì§€ ë´ë“œë¦´ê²Œìš”. ì¹´ë©”ë¼ë¥¼ ì ì‹œ ë°”ë¼ë´ì£¼ì„¸ìš”.")
            emotion = detect_emotion_yolo(show=False)
            if emotion in ['happy', 'sad']:
                if emotion == 'happy':
                    speak(f"ê¸°ë¶„ì´ ì¢‹ì•„ ë³´ì´ë„¤ìš”! ({emotion})")
                elif emotion == 'sad':
                    speak(f"ê¸°ë¶„ì´ ì•ˆ ì¢‹ì•„ ë³´ì´ë„¤ìš”. ë¬´ìŠ¨ ì¼ ìˆì–´ìš”? ({emotion})")
            else:
                speak("ì–¼êµ´ì„ ì œëŒ€ë¡œ ê°ì§€í•˜ì§€ ëª»í–ˆì–´ìš”.")

        elif "ìŒì•…" in user_text or "ë…¸ë˜" in user_text:
            speak("ìŒì•…ì„ ì¬ìƒí• ê²Œìš”.")
            play_music()

            # ìŒì•… ì¬ìƒ ì¤‘ ë©ˆì¶¤ ê°ì§€ ë£¨í”„
            while True:
                path = listen_audio()
                cleaned = clean_audio(path)
                cmd = transcribe_audio(cleaned).lower()
                if "êº¼ì¤˜" in cmd:
                    stop_music()
                    speak("ìŒì•…ì„ ëŒê²Œìš”.")
                    break
                else:
                    speak_nonblocking("ì£„ì†¡í•´ìš”, ì˜ ëª» ì•Œì•„ ë“¤ì—ˆì–´ìš”.")
                    # ìŒì•… ê³„ì† ìœ ì§€ (ì¤‘ë‹¨ ì•ˆ ë¨!)

        else:
            response = generate_response(user_text)
            speak(response)

        # ë‹¤ìŒ ì§ˆë¬¸ ìœ ë„
        speak("ë” í•„ìš”í•œ ê±° ìˆìœ¼ì‹ ê°€ìš”?")


if __name__ == "__main__":
    main()
